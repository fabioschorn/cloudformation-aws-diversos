AWSTemplateFormatVersion: "2010-09-09"
Description: >
  Creates an S3 bucket "vm-analysis-project-poc01" plus empty objects
  to visually represent folders. Also creates a Lambda to process only
  the most recent CSV in 'input-unprocessed/', skipping 8 lines, then
  splitting into PCI vs non-PCI. Manual invocation only (no triggers).

Resources:
  ##########################################################
  # 1) S3 Bucket
  ##########################################################
  VMAnalysisBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: vm-analysis-project-poc03

  ##########################################################
  # 2) Custom Resource Lambda: Creates "folders" as empty objects
  ##########################################################
  S3FolderCreatorLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "S3FolderCreatorRole-${AWS::StackName}"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: 
                - lambda.amazonaws.com
            Action: "sts:AssumeRole"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3FolderCreatorPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:DeleteObject
                  - s3:ListBucket
                Resource:
                  - !GetAtt VMAnalysisBucket.Arn
                  - !Sub "${VMAnalysisBucket.Arn}/*"

  S3FolderCreatorLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "S3FolderCreator-${AWS::StackName}"
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt S3FolderCreatorLambdaRole.Arn
      Timeout: 60
      Code:
        ZipFile: |
          import json
          import boto3
          import urllib3
          import os

          s3 = boto3.client('s3')

          # Minimal cfnresponse alternative using urllib3
          SUCCESS = "SUCCESS"
          FAILED = "FAILED"
          http = urllib3.PoolManager()

          def send_response(event, context, responseStatus, responseData, physicalResourceId=None, noEcho=False):
              responseUrl = event['ResponseURL']
              print("CFN response URL:", responseUrl)

              responseBody = {
                  'Status': responseStatus,
                  'Reason': f"See the details in CloudWatch Log Stream: {context.log_stream_name}",
                  'PhysicalResourceId': physicalResourceId or context.log_stream_name,
                  'StackId': event['StackId'],
                  'RequestId': event['RequestId'],
                  'LogicalResourceId': event['LogicalResourceId'],
                  'NoEcho': noEcho,
                  'Data': responseData
              }

              json_responseBody = json.dumps(responseBody)
              print("Response body:\n", json_responseBody)

              try:
                  headers = {'content-type': '', 'content-length': str(len(json_responseBody))}
                  response = http.request('PUT', responseUrl, body=json_responseBody, headers=headers)
                  print("CFN response status:", response.status)
              except Exception as e:
                  print("Failed to send CFN response:", e)

          def lambda_handler(event, context):
              print("Event:", json.dumps(event))

              # Some properties from the event
              request_type = event['RequestType']
              props = event['ResourceProperties']
              bucket_name = props['BucketName']
              folders = props.get('Folders', [])

              if request_type == 'Delete':
                  # Optionally delete the empty objects
                  # We do not strictly need to remove them, but let's do so to be tidy.
                  try:
                      for folder in folders:
                          s3.delete_object(Bucket=bucket_name, Key=folder)
                  except Exception as e:
                      print("Warning: Could not delete object", e)

                  send_response(event, context, SUCCESS, {"Status": "Folders deleted"})
                  return

              if request_type in ['Create', 'Update']:
                  # Put empty objects to emulate "folders"
                  try:
                      for folder in folders:
                          s3.put_object(Bucket=bucket_name, Key=folder)
                  except Exception as e:
                      print("Error creating folder objects:", e)
                      send_response(event, context, FAILED, {"Error": str(e)})
                      return

                  send_response(event, context, SUCCESS, {"Status": "Folders created"})
                  return

  # Now define the custom resource that calls the above Lambda
  S3FoldersCreationCR:
    Type: AWS::CloudFormation::CustomResource
    Properties:
      ServiceToken: !GetAtt S3FolderCreatorLambda.Arn
      BucketName: !Ref VMAnalysisBucket
      Folders:
        - "input-unprocessed/"
        - "output-processed/pci/"
        - "output-processed/non-pci/"

  ##########################################################
  # 3) IAM Role for the main CSV-processing Lambda
  ##########################################################
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "VMAnalysisLambdaRole-${AWS::StackName}"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: VMAnalysisS3Access
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:PutObject
                Resource:
                  - !GetAtt VMAnalysisBucket.Arn
                  - !Sub "${VMAnalysisBucket.Arn}/*"

  ##########################################################
  # 4) Main Lambda Function (Processing CSV)
  ##########################################################
  VMAnalysisLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "VMAnalysisLambda-${AWS::StackName}"
      Runtime: python3.9
      Role: !GetAtt LambdaExecutionRole.Arn
      Handler: index.lambda_handler
      Timeout: 300
      Code:
        ZipFile: |
          import boto3
          import csv
          import io
          import os
          from datetime import datetime
          import uuid

          s3 = boto3.client('s3')

          def lambda_handler(event, context):
              """
              Lambda that:
              1) Identifies the single most recent CSV file under 'input-unprocessed/' in the S3 bucket.
              2) Skips first 8 lines
              3) Reads the rest with semicolon (';') delimiter
              4) Keeps columns:
                 [IP, DNS, OS, QID, Title, Severity, CVE ID, Vendor Reference,
                  Threat, Impact, Solution, PCI Vuln, Category]
              5) Splits output into 2 CSVs (semicolon-delimited):
                 => 'PCI Vuln' == 'yes'  -> 'output-processed/pci/'
                 => otherwise           -> 'output-processed/non-pci/'
              6) No triggers; must be invoked manually (via console or CLI).

              event can include:
                {
                  "bucket": "<bucket-name>"   [optional, default = 'vm-analysis-project-poc01']
                }
              """

              bucket_name = event.get("bucket", os.environ.get("BUCKET_NAME", "vm-analysis-project-poc01"))
              prefix = "input-unprocessed/"

              # List objects in the prefix
              response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
              if "Contents" not in response:
                  print(f"No objects found at s3://{bucket_name}/{prefix}")
                  return {"status": "No files found"}

              # Identify the single MOST RECENT .csv file
              recent_obj = None
              for obj in response["Contents"]:
                  key = obj["Key"]
                  # skip the prefix object itself or non-csv
                  if key == prefix:
                      continue
                  if not key.lower().endswith(".csv"):
                      continue
                  if (recent_obj is None) or (obj["LastModified"] > recent_obj["LastModified"]):
                      recent_obj = obj

              if recent_obj is None:
                  print("No CSV files found in input-unprocessed/. Exiting.")
                  return {"status": "No CSV found"}

              # Download the most recent CSV
              key = recent_obj["Key"]
              print(f"Processing MOST RECENT CSV => s3://{bucket_name}/{key}")

              resp = s3.get_object(Bucket=bucket_name, Key=key)
              raw_data = resp["Body"].read().decode("utf-8").splitlines()

              # Skip first 8 lines
              data_without_headers = raw_data[8:]

              # Prepare a CSV DictReader using semicolon delimiter
              reader = csv.DictReader(data_without_headers, delimiter=';')

              # Define the columns we care about
              wanted_columns = [
                  "IP",
                  "DNS",
                  "OS",
                  "QID",
                  "Title",
                  "Severity",
                  "CVE ID",
                  "Vendor Reference",
                  "Threat",
                  "Impact",
                  "Solution",
                  "PCI Vuln",
                  "Category"
              ]

              # Prepare two in-memory CSVs
              pci_output = io.StringIO()
              nonpci_output = io.StringIO()

              pci_writer = csv.DictWriter(pci_output, fieldnames=wanted_columns, delimiter=';')
              nonpci_writer = csv.DictWriter(nonpci_output, fieldnames=wanted_columns, delimiter=';')

              # Write headers
              pci_writer.writeheader()
              nonpci_writer.writeheader()

              # Process each row, keep needed columns, check PCI Vuln
              for row in reader:
                  filtered_row = {}
                  for col in wanted_columns:
                      filtered_row[col] = row.get(col, "").strip()

                  pci_value = filtered_row["PCI Vuln"].lower()
                  if pci_value == "yes":
                      pci_writer.writerow(filtered_row)
                  else:
                      nonpci_writer.writerow(filtered_row)

              # Convert in-memory CSV to bytes
              pci_bytes = pci_output.getvalue().encode("utf-8")
              nonpci_bytes = nonpci_output.getvalue().encode("utf-8")

              # Build unique filenames
              timestamp = datetime.utcnow().strftime("%Y%m%d-%H%M%S")
              random_str = uuid.uuid4().hex[:8]

              pci_output_key = f"output-processed/pci/{timestamp}-{random_str}-pci.csv"
              nonpci_output_key = f"output-processed/non-pci/{timestamp}-{random_str}-non-pci.csv"

              # Upload the results
              s3.put_object(
                  Bucket=bucket_name,
                  Key=pci_output_key,
                  Body=pci_bytes
              )
              s3.put_object(
                  Bucket=bucket_name,
                  Key=nonpci_output_key,
                  Body=nonpci_bytes
              )

              print(f"Uploaded PCI file -> s3://{bucket_name}/{pci_output_key}")
              print(f"Uploaded non-PCI -> s3://{bucket_name}/{nonpci_output_key}")

              return {"status": "Completed processing the most recent CSV"}

Outputs:
  BucketName:
    Description: "S3 Bucket created for VM Analysis Project"
    Value: !Ref VMAnalysisBucket

  FoldersCreated:
    Description: "Status of the S3FoldersCreationCR custom resource"
    Value: !GetAtt S3FoldersCreationCR.LogicalResourceId

  VMAnalysisLambdaName:
    Description: "Name of the CSV-processing Lambda function"
    Value: !Ref VMAnalysisLambda

  VMAnalysisLambdaArn:
    Description: "ARN of the CSV-processing Lambda function"
    Value: !GetAtt VMAnalysisLambda.Arn