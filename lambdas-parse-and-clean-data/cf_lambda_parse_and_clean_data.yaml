AWSTemplateFormatVersion: "2010-09-09"
Description: >
  Creates an S3 bucket "vm-analysis-project-poc01" with the specified folder structure,
  plus a Lambda that processes ONLY the most recent CSV in input-unprocessed by skipping 
  8 lines and splitting into PCI vs non-PCI. The Lambda must be invoked manually (no triggers).

Resources:
  ###############################################
  # 1) S3 Bucket
  ###############################################
  VMAnalysisBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: "vm-analysis-project-poc01"

  ###############################################
  # 2) Create "folders" (actually empty objects)
  ###############################################
  InputUnprocessedFolder:
    Type: AWS::S3::Object
    Properties:
      Bucket: !Ref VMAnalysisBucket
      Key: "input-unprocessed/"
      ContentType: "application/x-directory"

  OutputProcessedPciFolder:
    Type: AWS::S3::Object
    Properties:
      Bucket: !Ref VMAnalysisBucket
      Key: "output-processed/pci/"
      ContentType: "application/x-directory"

  OutputProcessedNonPciFolder:
    Type: AWS::S3::Object
    Properties:
      Bucket: !Ref VMAnalysisBucket
      Key: "output-processed/non-pci/"
      ContentType: "application/x-directory"

  ###############################################
  # 3) IAM Role for Lambda (with S3 read/write)
  ###############################################
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "VMAnalysisLambdaRole-${AWS::StackName}"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: VMAnalysisS3Access
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:PutObject
                Resource:
                  - !GetAtt VMAnalysisBucket.Arn
                  - !Sub "${VMAnalysisBucket.Arn}/*"

  ###############################################
  # 4) Lambda Function (Python code inline)
  ###############################################
  VMAnalysisLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "VMAnalysisLambda-${AWS::StackName}"
      Runtime: python3.9
      Role: !GetAtt LambdaExecutionRole.Arn
      Handler: index.lambda_handler
      Timeout: 300
      Code:
        ZipFile: |
          import boto3
          import csv
          import io
          import os
          from datetime import datetime
          import uuid

          s3 = boto3.client('s3')

          def lambda_handler(event, context):
              """
              Lambda that:
              1) Identifies the single most recent CSV file under 'input-unprocessed/' in the S3 bucket.
              2) Skips first 8 lines
              3) Reads the rest with semicolon (';') delimiter
              4) Keeps columns: 
                 [IP, DNS, OS, QID, Title, Severity, CVE ID, Vendor Reference,
                  Threat, Impact, Solution, PCI Vuln, Category]
              5) Splits output into 2 CSVs (semicolon-delimited):
                 => 'PCI Vuln' == 'yes'  -> uploaded to 'output-processed/pci/'
                 => otherwise           -> uploaded to 'output-processed/non-pci/'
              6) No triggers; must be invoked manually (via console or CLI).

              event can include:
                {
                  "bucket": "<bucket-name>"   [optional, default to env var or "vm-analysis-project-poc01"]
                }
              """

              # Bucket can come from event or default
              bucket_name = event.get("bucket", os.environ.get("BUCKET_NAME", "vm-analysis-project-poc01"))

              prefix = "input-unprocessed/"
              response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
              if "Contents" not in response:
                  print(f"No objects found at s3://{bucket_name}/{prefix}")
                  return {"status": "No files found"}

              # 1. Identify the single MOST RECENT .csv file
              recent_obj = None
              for obj in response["Contents"]:
                  key = obj["Key"]
                  if key == prefix:  # skip the folder object
                      continue
                  if not key.lower().endswith(".csv"):
                      continue

                  # Compare LastModified to track the most recent
                  if (recent_obj is None) or (obj["LastModified"] > recent_obj["LastModified"]):
                      recent_obj = obj

              if recent_obj is None:
                  print("No CSV files found in input-unprocessed/. Exiting.")
                  return {"status": "No CSV found"}

              # 2. We now have the single most recent CSV
              key = recent_obj["Key"]
              print(f"Processing MOST RECENT CSV => s3://{bucket_name}/{key}")

              response = s3.get_object(Bucket=bucket_name, Key=key)
              raw_data = response["Body"].read().decode("utf-8").splitlines()

              # Skip first 8 lines
              data_without_headers = raw_data[8:]  

              # Prepare a CSV DictReader using semicolon delimiter
              reader = csv.DictReader(data_without_headers, delimiter=';')

              # Define columns we care about
              wanted_columns = [
                  "IP",
                  "DNS",
                  "OS",
                  "QID",
                  "Title",
                  "Severity",
                  "CVE ID",
                  "Vendor Reference",
                  "Threat",
                  "Impact",
                  "Solution",
                  "PCI Vuln",
                  "Category"
              ]

              # We'll write two in-memory CSV buffers
              pci_output = io.StringIO()
              nonpci_output = io.StringIO()

              pci_writer = csv.DictWriter(pci_output, fieldnames=wanted_columns, delimiter=';')
              nonpci_writer = csv.DictWriter(nonpci_output, fieldnames=wanted_columns, delimiter=';')

              # Write headers
              pci_writer.writeheader()
              nonpci_writer.writeheader()

              # Process each row, pick needed columns, classify PCI or non-PCI
              for row in reader:
                  filtered_row = {}
                  for col in wanted_columns:
                      filtered_row[col] = row.get(col, "").strip()

                  # Check PCI Vuln
                  pci_value = filtered_row["PCI Vuln"].lower()
                  if pci_value == "yes":
                      pci_writer.writerow(filtered_row)
                  else:
                      nonpci_writer.writerow(filtered_row)

              # Convert to bytes
              pci_bytes = pci_output.getvalue().encode("utf-8")
              nonpci_bytes = nonpci_output.getvalue().encode("utf-8")

              # Unique file names
              timestamp = datetime.utcnow().strftime("%Y%m%d-%H%M%S")
              random_str = uuid.uuid4().hex[:8]

              pci_output_key = f"output-processed/pci/{timestamp}-{random_str}-pci.csv"
              nonpci_output_key = f"output-processed/non-pci/{timestamp}-{random_str}-non-pci.csv"

              # Upload results
              s3.put_object(
                  Bucket=bucket_name,
                  Key=pci_output_key,
                  Body=pci_bytes
              )
              s3.put_object(
                  Bucket=bucket_name,
                  Key=nonpci_output_key,
                  Body=nonpci_bytes
              )

              print(f"Uploaded PCI file -> s3://{bucket_name}/{pci_output_key}")
              print(f"Uploaded non-PCI -> s3://{bucket_name}/{nonpci_output_key}")

              return {"status": "Completed processing the most recent CSV"}

Outputs:
  BucketName:
    Description: "S3 Bucket created for VM Analysis Project"
    Value: !Ref VMAnalysisBucket

  LambdaName:
    Description: "Name of the Lambda function"
    Value: !Ref VMAnalysisLambda

  LambdaArn:
    Description: "ARN of the Lambda function"
    Value: !GetAtt VMAnalysisLambda.Arn