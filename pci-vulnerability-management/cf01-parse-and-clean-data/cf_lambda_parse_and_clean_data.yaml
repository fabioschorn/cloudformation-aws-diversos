AWSTemplateFormatVersion: "2010-09-09"
Description: >
  Creates ONLY the IAM Role and the Lambda function for processing Qualys CSV.
  Assumes an S3 bucket "vm-analysis-project-poc02" already exists with:
    - input-unprocessed/
    - output-processed/
    - output-processed/pci/
    - output-processed/non-pci/

Resources:
  ##########################################################
  # 1) IAM Role for the main CSV-processing Lambda
  ##########################################################
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "VMAnalysisLambdaRole-${AWS::StackName}"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action: "sts:AssumeRole"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: VMAnalysisLambdaPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:PutObject
                Resource:
                  - !Sub "arn:aws:s3:::vm-analysis-project-poc02"
                  - !Sub "arn:aws:s3:::vm-analysis-project-poc02/*"
              - Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:Encrypt
                  - kms:ReEncrypt*
                  - kms:GenerateDataKey*
                  - kms:DescribeKey
                Resource: '*'

  ##########################################################
  # 2) Main Lambda Function (Processing CSV)
  ##########################################################
  VMAnalysisLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "VMAnalysisLambda-${AWS::StackName}"
      Runtime: python3.9
      Role: !GetAtt LambdaExecutionRole.Arn
      Handler: index.lambda_handler
      Timeout: 300
      Environment:
        Variables:
          BUCKET_NAME: "vm-analysis-project-poc02"
      Code:
        ZipFile: |
          import boto3
          import csv
          import io
          import os
          from datetime import datetime
          import uuid

          s3 = boto3.client('s3')

          def lambda_handler(event, context):
              """
              Lambda that:
              1) Identifies the single most recent CSV file under 'input-unprocessed/' in the S3 bucket.
              2) Skips first 8 lines
              3) Reads the rest with semicolon (';') delimiter
              4) Keeps columns:
                 [IP, DNS, OS, QID, Title, Severity, CVE ID, Vendor Reference,
                  Threat, Impact, Solution, PCI Vuln, Category]
              5) Splits output into 2 CSVs (semicolon-delimited):
                 => 'PCI Vuln' == 'yes'  -> 'output-processed/pci/'
                 => otherwise           -> 'output-processed/non-pci/'
              6) No triggers; must be invoked manually (via console or CLI).

              event can include:
                {
                  "bucket": "<bucket-name>"   [optional, default = 'vm-analysis-project-poc01']
                }
              """

              bucket_name = event.get("bucket", os.environ.get("BUCKET_NAME", "vm-analysis-project-poc01"))
              prefix = "input-unprocessed/"

              # List objects in the prefix
              response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
              if "Contents" not in response:
                  print(f"No objects found at s3://{bucket_name}/{prefix}")
                  return {"status": "No files found"}

              # Identify the single MOST RECENT .csv file
              recent_obj = None
              for obj in response["Contents"]:
                  key = obj["Key"]
                  # skip the prefix object itself or non-csv
                  if key == prefix:
                      continue
                  if not key.lower().endswith(".csv"):
                      continue
                  if (recent_obj is None) or (obj["LastModified"] > recent_obj["LastModified"]):
                      recent_obj = obj

              if recent_obj is None:
                  print("No CSV files found in input-unprocessed/. Exiting.")
                  return {"status": "No CSV found"}

              # Download the most recent CSV
              key = recent_obj["Key"]
              print(f"Processing MOST RECENT CSV => s3://{bucket_name}/{key}")

              resp = s3.get_object(Bucket=bucket_name, Key=key)
              raw_data = resp["Body"].read().decode("utf-8").splitlines()

              # Skip first 8 lines
              data_without_headers = raw_data[8:]

              # Prepare a CSV DictReader using semicolon delimiter
              reader = csv.DictReader(data_without_headers, delimiter=';')

              # Define the columns we care about
              wanted_columns = [
                  "IP",
                  "DNS",
                  "OS",
                  "QID",
                  "Title",
                  "Severity",
                  "Port",
                  "CVE ID",
                  "Vendor Reference",
                  "Threat",
                  "Impact",
                  "Solution",
                  "PCI Vuln",
                  "Category"
              ]

              # Prepare two in-memory CSVs
              pci_output = io.StringIO()
              nonpci_output = io.StringIO()

              pci_writer = csv.DictWriter(pci_output, fieldnames=wanted_columns, delimiter=';')
              nonpci_writer = csv.DictWriter(nonpci_output, fieldnames=wanted_columns, delimiter=';')

              # Write headers
              pci_writer.writeheader()
              nonpci_writer.writeheader()

              # Process each row, keep needed columns, check PCI Vuln
              for row in reader:
                  filtered_row = {}
                  for col in wanted_columns:
                      filtered_row[col] = row.get(col, "").strip()

                  pci_value = filtered_row["PCI Vuln"].lower()
                  if pci_value == "yes":
                      pci_writer.writerow(filtered_row)
                  else:
                      nonpci_writer.writerow(filtered_row)

              # Convert in-memory CSV to bytes
              pci_bytes = pci_output.getvalue().encode("utf-8")
              nonpci_bytes = nonpci_output.getvalue().encode("utf-8")

              # Build unique filenames
              timestamp = datetime.utcnow().strftime("%Y%m%d-%H%M%S")
              random_str = uuid.uuid4().hex[:8]

              pci_output_key = f"output-processed/pci/{timestamp}-{random_str}-pci.csv"
              nonpci_output_key = f"output-processed/non-pci/{timestamp}-{random_str}-non-pci.csv"

              # Upload the results
              s3.put_object(
                  Bucket=bucket_name,
                  Key=pci_output_key,
                  Body=pci_bytes
              )
              s3.put_object(
                  Bucket=bucket_name,
                  Key=nonpci_output_key,
                  Body=nonpci_bytes
              )

              print(f"Uploaded PCI file -> s3://{bucket_name}/{pci_output_key}")
              print(f"Uploaded non-PCI -> s3://{bucket_name}/{nonpci_output_key}")

              return {"status": "Completed processing the most recent CSV"}

  # 3) Lambda Permission for S3 to Invoke the Function (s3:ObjectCreated:*)
  VMAnalysisLambdaS3InvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: "lambda:InvokeFunction"
      FunctionName: !GetAtt VMAnalysisLambda.Arn
      Principal: "s3.amazonaws.com"
      SourceAccount: !Ref "AWS::AccountId"
      SourceArn: !Sub "arn:aws:s3:::vm-analysis-project-poc02"

  # 4) S3 Bucket Notification (trigger) for 'input-unprocessed/' prefix and .csv files (Manual config after deploy)
  VMAnalysisLambdaS3Trigger:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: "vm-analysis-project-poc02"
      NotificationConfiguration:
        LambdaConfigurations:
          - Event: "s3:ObjectCreated:*"
            Function: !GetAtt VMAnalysisLambda.Arn

Outputs:
  LambdaFunctionName:
    Description: "Name of the CSV-processing Lambda function"
    Value: !Ref VMAnalysisLambda

  LambdaFunctionArn:
    Description: "ARN of the CSV-processing Lambda function"
    Value: !GetAtt VMAnalysisLambda.Arn